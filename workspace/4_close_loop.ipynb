{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice “closing the feedback loop”\n",
    "\n",
    "When there are no natural ground truth labels, we need to explicitly “close the feedback loop”:\n",
    "\n",
    "-   in order to evaluate how well our model does in production, versus in offline evaluation on a held-out test set,\n",
    "-   and also to get new “production data” on which to re-train the model when its performance degrades.\n",
    "\n",
    "For example, with this food type classifier, once it is deployed to “real” users:\n",
    "\n",
    "-   We could set aside a portion of production data for a human to label.\n",
    "-   We could set aside samples where the model has low confidence in its prediction, for a human to label. These extra-difficult samples are especially useful for re-training.\n",
    "-   We could allow users to give explicit feedback about whether the label assigned to their image is correct or not. This feedback may be sparse (some users won’t bother giving feedback even if the label is wrong) and noisy (some users may give incorrect feedback). We can get human annotators to label this data, too.\n",
    "-   We could allow users to explicitly label their images, by changing the label that is assigned by the classifier. This feedback may be sparse (some users won’t bother giving feedback even if the label is wrong) and noisy (some users may give incorrect feedback).\n",
    "\n",
    "We’re going to try out all of these options!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Architecture and Data Flow\n",
    "\n",
    "Here’s how all the components work together:\n",
    "\n",
    "1.  **Flask Service**: This is our web application that users interact with\n",
    "\n",
    "2.  **FastAPI Service**: This provides the machine learning prediction endpoint that the Flask app calls\n",
    "\n",
    "3.  **MinIO Object Store**: This stores all our data including the Source and Target Storage\n",
    "\n",
    "4.  **Label Studio**: This is our annotation platform\n",
    "\n",
    "Data flows like this:\n",
    "\n",
    "`User → Flask → FastAPI → Flask → Source Storage (labelstudio/tasks/) → Label Studio → Target Storage (labelstudio/output/)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up Label Studio for Annotation\n",
    "\n",
    "Before we get started, we have to setup Label Studio for managing the annotation tasks.\n",
    "\n",
    "A project in Label Studio is a workspace where you organize your data annotation tasks. Each project has:\n",
    "\n",
    "-   A specific labeling interface (the UI annotators interact with)\n",
    "-   Its own dataset of items to be labeled\n",
    "-   Unique annotation guidelines and settings\n",
    "\n",
    "Projects help you organize different annotation tasks separately. For example, the “Random Sampling Review” project will specifically focus on reviewing and correcting random samples of food classification predictions.\n",
    "\n",
    "For the first project, lets create it via the Label Studio UI.\n",
    "\n",
    "Access Label Studio UI: Visit http://{node-public-ip}:8080 and login with below credentials :\n",
    "\n",
    "-   Username: gourmetgramuser@gmail.com\n",
    "-   Password: gourmetgrampassword\n",
    "\n",
    "Once you enter the application :\n",
    "\n",
    "-   Click on `Create Project`\n",
    "-   In the Project Name tab, Enter Project Name as `Random Sampling Review` and Description as `Review and correct random sampled food classification predictions`\n",
    "-   In the Labelling Setup tab, click on `Custom template` option and paste the configuration present in workspace/label_studio_config.xml. Verify the UI Preview appears correctly on the right\n",
    "-   Click on Save button on the top right\n",
    "\n",
    "Now, let’s connect this Project to an input storage.\n",
    "\n",
    "-   In the `Random Sampling Review` Project, click on Settings\n",
    "-   In the Cloud Storage tab, click on “Add Source Storage.” This connects Label Studio to the location where your annotation task files are stored.\n",
    "-   Here, add the Storage title as `Source Storage`, Bucket Name as `labelstudio`, Bucket Prefix as `tasks/randomsampled`, Region Name as `us-east-1`, S3 Endpoint as `http://minio:9000`, Access Key ID as `minioadmin`, Secret Access Key as `minioadmin`\n",
    "-   Click on Check Connection to validate, and then click on Add Storage\n",
    "-   Click on Sync Storage to sync the input storage with label studio\n",
    "-   Now, in the `Random Sampling Review` Project, you’ll find a sample task that you can go through.\n",
    "\n",
    "Similarly, let’s connect this Project to an output storage.\n",
    "\n",
    "-   In the Cloud Storage tab, click on “Add Target Storage.” This establishes a designated location where Label Studio will save all completed annotations.\n",
    "-   Here, add the Storage title as `Target Storage`, Bucket Name as `labelstudio`, Bucket Prefix as `output/randomsampled`, Region Name as `us-east-1`, S3 Endpoint as `http://minio:9000`, Access Key ID as `minioadmin`, Secret Access Key as `minioadmin`\n",
    "-   Click on Check Connection to validate, and then click on Add Storage\n",
    "-   Click on Sync Storage to sync the output storage with label studio\n",
    "\n",
    "With Label Studio configured for our first project, let’s move forward with the annotation workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set aside data for a human to label\n",
    "\n",
    "This stage involves storing user-submitted images in the Production bucket within our MinIO Object Store.\n",
    "\n",
    "In order to do this, let’s modify the flask application.\n",
    "\n",
    "Inside the SSH session :\n",
    "\n",
    "-   Add `s3fs` to requirements.txt in the gourmetgram folder\n",
    "\n",
    "``` bash\n",
    "nano /home/cc/eval-loop-chi/gourmetgram/requirements.txt\n",
    "```\n",
    "\n",
    "-   Copy utils folder into gourmetgram folder\n",
    "\n",
    "``` bash\n",
    "cp -r /home/cc/eval-loop-chi/gourmetgram_utils /home/cc/eval-loop-chi/gourmetgram/gourmetgram_utils\n",
    "```\n",
    "\n",
    "-   Modify the contents of app.py in gourmetgram folder using below command.\n",
    "\n",
    "``` bash\n",
    "nano /home/cc/eval-loop-chi/gourmetgram/app.py\n",
    "```\n",
    "\n",
    "In app.py,\n",
    "\n",
    "Add these imports at the top of the file:\n",
    "\n",
    "``` python\n",
    "import s3fs\n",
    "import json\n",
    "import datetime\n",
    "import uuid\n",
    "#Include jsonify here\n",
    "from flask import Flask, redirect, url_for, request, render_template, jsonify\n",
    "from gourmetgram_utils.storage import store_prediction_in_tracking\n",
    "```\n",
    "\n",
    "Initialize S3 Filesystem and a dictionary to store predictions:\n",
    "\n",
    "``` python\n",
    "# Initalize s3fs \n",
    "fs = s3fs.S3FileSystem(endpoint_url=\"http://minio:9000\",key=\"minioadmin\",secret=\"minioadmin\",use_ssl=False)\n",
    "\n",
    "classes = np.array([\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\",\n",
    "    \"Meat\", \"Noodles/Pasta\", \"Rice\", \"Seafood\", \"Soup\",\n",
    "    \"Vegetable/Fruit\"])\n",
    "\n",
    "# Dictionary to store predictions\n",
    "current_predictions = {}\n",
    "```\n",
    "\n",
    "Update the upload() function to save images and prediction details :\n",
    "\n",
    "``` python\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    preds = None\n",
    "    if request.method == 'POST':\n",
    "        f = request.files['file']\n",
    "        filename = secure_filename(f.filename)\n",
    "        f.save(os.path.join(app.instance_path, 'uploads', filename))\n",
    "        img_path = os.path.join(app.instance_path, 'uploads', filename)\n",
    "       \n",
    "        preds, probs = request_fastapi(img_path)\n",
    "        if preds:\n",
    "            pred_index = np.where(classes == preds)[0][0]\n",
    "            \n",
    "            # Format the class directory name with the index\n",
    "            class_dir = f\"class_{pred_index:02d}\"\n",
    "            \n",
    "            # Create the S3 path\n",
    "            bucket_name = \"production\"\n",
    "            s3_path = f\"{bucket_name}/{class_dir}/{secure_filename(f.filename)}\"\n",
    "            \n",
    "            # Upload the file to S3/MinIO\n",
    "            fs.put(img_path, s3_path)\n",
    "\n",
    "            prediction_id = str(uuid.uuid4())\n",
    "\n",
    "            current_predictions[prediction_id] = {\n",
    "                \"prediction_id\": prediction_id,\n",
    "                \"filename\": filename,\n",
    "                \"prediction\": preds,\n",
    "                \"confidence\": probs,\n",
    "                \"image_url\": f\"http://localhost:9000/production/{class_dir}/{filename}\",\n",
    "                \"class_dir\": class_dir,\n",
    "                \"sampled\" : False\n",
    "            }\n",
    "\n",
    "            # Store prediction in tracking\n",
    "            store_prediction_in_tracking(fs, current_predictions[prediction_id])\n",
    "            \n",
    "            return f'<button type=\"button\" class=\"btn btn-info btn-sm\">{preds}</button>'\n",
    "    \n",
    "    return '<a href=\"#\" class=\"badge badge-warning\">Warning</a>'\n",
    "```\n",
    "\n",
    "Rebuild the Flask Container:\n",
    "\n",
    "``` bash\n",
    "# Rebuild the Flask container with the updated app.py\n",
    "docker-compose -f /home/cc/eval-loop-chi/docker/docker-compose-feedback.yaml up flask --build\n",
    "```\n",
    "\n",
    "Our first feedback loop method randomly selects production images for human annotation.\n",
    "\n",
    "#### Testing the Feedback Loop\n",
    "\n",
    "-   Go to http://{public-node-ip}:5000\n",
    "-   Upload test images from the /data/food11 folder\n",
    "-   Initiate the random sampling process by executing the scheduler’s sampling script in SSH\n",
    "\n",
    "``` bash\n",
    "docker exec scheduler python /app/scripts/random_sampling.py\n",
    "```\n",
    "\n",
    "-   After execution, the sampling script automatically generates task JSONs and places them in the /tasks/randomsampled folder within the labelstudio bucket (your configured Source Storage). Visit MinIO object store at http://{public-node-ip}:9001 to examine the structure of these task files, which contain the image references and metadata needed for the annotation process.\n",
    "-   Now, go to Label Studio and check the Random Sampling Review Project. You’ll notice there are no tasks displayed yet. This is because Label Studio doesn’t automatically synchronize with the source storage - it won’t scan for new tasks until we explicitly trigger a sync operation either through the GUI or the Label Studio API.\n",
    "-   Let’s do it via the GUI this time. Head to settings and in the Cloud Storage tab, Click on Sync Storage for source storage. Now, you’ll be able to see the tasks in the project.\n",
    "-   Go ahead and complete the labelling tasks for the randomly sampled images.\n",
    "-   After completing the labelling tasks, you need to send your labelling results back to the MinIO. So in Settings -\\>Cloud Storage tab, Click on Sync Storage for target storage. Now, you will find the labelling results in the /output/randomsampled folder within the labelstudio bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set aside samples for which model has low confidence\n",
    "\n",
    "Our second method identifies images where the model has low confidence in its prediction, making them valuable for retraining.\n",
    "\n",
    "Using the UI everytime to create and setup Label Studio Projects is cumbersome. For this reason, we can use Label Studio API to directly interact with the application. Let’s try that to setup our 2nd project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests-\n",
    "\n",
    "LABEL_STUDIO_URL ='http://label-studio:8080'\n",
    "API_TOKEN = 'ab9927067c51ff279d340d7321e4890dc2841c4a'\n",
    "MINIO_ENDPOINT = 'http://minio:9000'\n",
    "MINIO_USER = 'minioadmin'\n",
    "MINIO_PASSWORD = 'minioadmin'\n",
    "\n",
    "PROJECT_CONFIG = {\n",
    "    \"title\": \"Low Confidence Review\",\n",
    "    \"description\": \"Review and correct low confidence food classification predictions\",\n",
    "    \"source_folder\": \"lowconfidence\",\n",
    "    \"target_folder\": \"lowconfidence\"\n",
    "}\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Token {API_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Project config files are in .xml format \n",
    "with open('label_studio_config.xml', 'r') as file:\n",
    "    label_config = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_project():\n",
    "    \"\"\"Create the Low Confidence Review project in Label Studio\"\"\"\n",
    "    \n",
    "    project_data = {\n",
    "        \"title\": PROJECT_CONFIG[\"title\"],\n",
    "        \"description\": PROJECT_CONFIG[\"description\"],\n",
    "        \"label_config\": label_config\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{LABEL_STUDIO_URL}/api/projects\",\n",
    "        headers=HEADERS,\n",
    "        json=project_data\n",
    "    )\n",
    "    \n",
    "    if response.status_code in [201, 200]:\n",
    "        project = response.json()\n",
    "        print(f\"Created project '{project['title']}' with ID {project['id']}\")\n",
    "        return project\n",
    "    else:\n",
    "        print(f\"Failed to create project: {response.status_code} {response.text}\")\n",
    "        return None\n",
    "\n",
    "project = create_project()\n",
    "project_id = project[\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_input_storage(project_id):\n",
    "    \"\"\"Connect S3 source storage for low confidence data to the project\"\"\"\n",
    "    folder_name = PROJECT_CONFIG[\"source_folder\"]\n",
    "    \n",
    "    storage_config = {\n",
    "        \"title\": f\"Source Storage - {folder_name}\",\n",
    "        \"description\": f\"S3 storage for {folder_name} tasks\",  \n",
    "        \"project\": project_id,\n",
    "        \"bucket\": \"labelstudio\",\n",
    "        \"prefix\": f\"tasks/{folder_name}/\",\n",
    "        \"aws_access_key_id\": MINIO_USER,\n",
    "        \"aws_secret_access_key\": MINIO_PASSWORD,\n",
    "        \"region_name\": \"us-east-1\",  \n",
    "        \"s3_endpoint\": MINIO_ENDPOINT\n",
    "    }\n",
    "    \n",
    "    # Create the storage connection\n",
    "    response = requests.post(\n",
    "        f\"{LABEL_STUDIO_URL}/api/storages/s3\",\n",
    "        headers=HEADERS,\n",
    "        json=storage_config\n",
    "    )\n",
    "    \n",
    "    if response.status_code in [201, 200]:\n",
    "        storage = response.json()\n",
    "        print(f\"Connected source storage for low confidence data to project {project_id}\")\n",
    "            \n",
    "        return storage\n",
    "    else:\n",
    "        print(f\"Failed to connect source storage: {response.status_code} {response.text}\")\n",
    "        return None\n",
    "\n",
    "input_storage = connect_input_storage(project_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_output_storage(project_id):\n",
    "    \"\"\"Connect S3 target storage for low confidence annotations to the project\"\"\"\n",
    "    folder_name = PROJECT_CONFIG[\"target_folder\"]\n",
    "    \n",
    "    storage_config = {\n",
    "        \"title\": f\"Target Storage - {folder_name}\",\n",
    "        \"description\": f\"S3 storage for exporting {folder_name} annotations\",\n",
    "        \"project\": project_id,\n",
    "        \"bucket\": \"labelstudio\",\n",
    "        \"prefix\": f\"output/{folder_name}\",\n",
    "        \"aws_access_key_id\": MINIO_USER,\n",
    "        \"aws_secret_access_key\": MINIO_PASSWORD,\n",
    "        \"region_name\": \"us-east-1\", \n",
    "        \"s3_endpoint\": MINIO_ENDPOINT,\n",
    "        \"can_delete_objects\": True,\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{LABEL_STUDIO_URL}/api/storages/export/s3\",\n",
    "        headers=HEADERS,\n",
    "        json=storage_config\n",
    "    )\n",
    "    \n",
    "    if response.status_code in [201, 200]:\n",
    "        storage = response.json()\n",
    "        print(f\"Connected target storage for low confidence annotations to project {project_id}\")\n",
    "        return storage\n",
    "    else:\n",
    "        print(f\"Failed to connect target storage: {response.status_code} {response.text}\")\n",
    "        return None\n",
    "\n",
    "output_storage = connect_output_storage(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, with our Project ready for low confidence predictions. Let’s modify our Flask application\n",
    "\n",
    "Use the below command to modify app.py :\n",
    "\n",
    "``` bash\n",
    "nano /home/cc/eval-loop-chi/gourmetgram/app.py\n",
    "```\n",
    "\n",
    "-   Import Task Creation Function for low confidence tasks\n",
    "\n",
    "Add this import to app.py:\n",
    "\n",
    "``` python\n",
    "from gourmetgram_utils.feedback_tasks import create_low_confidence_task\n",
    "```\n",
    "\n",
    "-   Update the upload() function in app.py to identify and send low confidence predictions for review based on a predefined threshold:\n",
    "\n",
    "``` python\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        f = request.files['file']\n",
    "        filename = secure_filename(f.filename)\n",
    "        img_path = os.path.join(app.instance_path, 'uploads', filename)\n",
    "        f.save(img_path)\n",
    "       \n",
    "        preds, probs = request_fastapi(img_path)\n",
    "        if preds:\n",
    "            pred_index = np.where(classes == preds)[0][0]\n",
    "            \n",
    "            # Format the class directory name with the index\n",
    "            class_dir = f\"class_{pred_index:02d}\"\n",
    "            \n",
    "            # Create the S3 path\n",
    "            bucket_name = \"production\"\n",
    "            s3_path = f\"{bucket_name}/{class_dir}/{filename}\"\n",
    "            \n",
    "            # Upload the file to S3/MinIO\n",
    "            fs.put(img_path, s3_path)\n",
    "\n",
    "            prediction_id = str(uuid.uuid4())\n",
    "            current_predictions[prediction_id] = {\n",
    "                \"prediction_id\": prediction_id,\n",
    "                \"filename\": filename,\n",
    "                \"prediction\": preds,\n",
    "                \"confidence\": probs,\n",
    "                \"image_url\": f\"http://localhost:9000/production/{class_dir}/{filename}\",\n",
    "                \"class_dir\": class_dir,\n",
    "                \"sampled\" : False\n",
    "            }\n",
    "\n",
    "            store_prediction_in_tracking(fs, current_predictions[prediction_id])\n",
    "\n",
    "            confidence_threshold = 0.7\n",
    "\n",
    "            if probs < confidence_threshold:\n",
    "                create_low_confidence_task(\n",
    "                    fs,\n",
    "                    image_url=current_predictions[prediction_id][\"image_url\"],\n",
    "                    predicted_class=preds,\n",
    "                    confidence=probs,\n",
    "                    filename=filename\n",
    "                )\n",
    "            \n",
    "            return f'<button type=\"button\" class=\"btn btn-info btn-sm\">{preds}</button>'\n",
    "    \n",
    "    return '<a href=\"#\" class=\"badge badge-warning\">Warning</a>'\n",
    "```\n",
    "\n",
    "-   Rebuild the Flask container\n",
    "\n",
    "``` bash\n",
    "# Rebuild the Flask container with the updated app.py\n",
    "docker-compose -f /home/cc/eval-loop-chi/docker/docker-compose-feedback.yaml up flask --build\n",
    "```\n",
    "\n",
    "#### Testing the Feedback Loop\n",
    "\n",
    "-   Go to http://{public-node-ip}:5000.\n",
    "-   Upload test images from the /data/lowconfidence folder in data.\n",
    "-   With this, task JSONs are inserted in the /tasks/lowconfidence folder in labelstudio bucket (Source Storage).\n",
    "-   This time, for syncing the source storage, let’s use the label studio API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_import_storage(project_id):\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {API_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{LABEL_STUDIO_URL}/api/storages/s3?project={project_id}\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    storage_id = None\n",
    "    if response.status_code == 200:\n",
    "        storages = response.json()\n",
    "        if storages:\n",
    "            storage_id = storages[0][\"id\"]\n",
    "    \n",
    "    if not storage_id:\n",
    "        print(f\"Import storage not found in project {project_id}!\")\n",
    "        return False\n",
    "    \n",
    "    # Sync storage\n",
    "    sync_response = requests.post(\n",
    "        f\"{LABEL_STUDIO_URL}/api/storages/s3/{storage_id}/sync\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if sync_response.status_code in [200, 201, 204]:\n",
    "        print(f\"Successfully synced import storage for (project ID {project_id})\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to sync import storage: {sync_response.status_code} {sync_response.text}\")\n",
    "        return False\n",
    "\n",
    "print((sync_import_storage(project_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Now check the Label Studio interface and navigate to the “Low Confidence Project.” You should see the tasks have been successfully synchronized and are now available for review.\n",
    "-   Proceed to complete the labeling tasks for these low confidence images. Your corrections will help improve the model’s accuracy on challenging cases.\n",
    "-   After completing the labeling tasks, you need to send your work back to MinIO. Run the code cell below to synchronize your completed labels with the target storage in MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_export_storage(project_id):\n",
    "    \n",
    "    # Get storage ID\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Token {API_TOKEN}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{LABEL_STUDIO_URL}/api/storages/export/s3?project={project_id}\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    storage_id = None\n",
    "    if response.status_code == 200:\n",
    "        storages = response.json()\n",
    "        if storages:\n",
    "            storage_id = storages[0][\"id\"]\n",
    "    \n",
    "    if not storage_id:\n",
    "        print(f\"Export storage not found in project {project_id}!\")\n",
    "        return False\n",
    "    \n",
    "    # Sync storage\n",
    "    sync_response = requests.post(\n",
    "        f\"{LABEL_STUDIO_URL}/api/storages/export/s3/{storage_id}/sync\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if sync_response.status_code in [200, 201, 204]:\n",
    "        print(f\"Successfully synced export storage (project ID {project_id})\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to sync export storage: {sync_response.status_code} {sync_response.text}\")\n",
    "        return False\n",
    "print((sync_export_storage(project_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get explicit feedback from users\n",
    "\n",
    "Our third method enables users to provide feedback when they think the model’s prediction is incorrect. This feedback may be sparse (some users won’t bother giving feedback even if the label is wrong) and noisy (some users may give incorrect feedback). We can get human annotators to label this data, too.\n",
    "\n",
    "Let’s use the same functions we created last time to create the user feedback labelling project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_CONFIG = {\n",
    "        \"title\": \"User Feedback Review\",\n",
    "        \"description\": \"Review and correct food classification based on user feedback\",\n",
    "        \"source_folder\": \"userfeedback\",\n",
    "        \"target_folder\": \"userfeedback\"\n",
    "}\n",
    "\n",
    "project = create_project()\n",
    "\n",
    "project_id = project[\"id\"]\n",
    "\n",
    "input_storage = connect_input_storage(project_id)\n",
    "\n",
    "output_storage = connect_output_storage(project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the below command to modify app.py :\n",
    "\n",
    "``` bash\n",
    "nano /home/cc/eval-loop-chi/gourmetgram/app.py\n",
    "```\n",
    "\n",
    "-   Import Task Creation Function for user feedback tasks and add the flag icon SVG\n",
    "\n",
    "``` python\n",
    "from gourmetgram_utils.feedback_tasks import create_user_feedback_task\n",
    "```\n",
    "\n",
    "-   Import SVG Icon and Update Upload Function to Include Feedback Button\n",
    "\n",
    "``` python\n",
    "with open('./images/flag-icon.svg', 'r') as f:\n",
    "    FLAG_SVG = f.read()\n",
    "\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        f = request.files['file']\n",
    "        filename = secure_filename(f.filename)\n",
    "        img_path = os.path.join(app.instance_path, 'uploads', filename)\n",
    "        f.save(img_path)\n",
    "       \n",
    "        preds, probs = request_fastapi(img_path)\n",
    "        if preds:\n",
    "            pred_index = np.where(classes == preds)[0][0]\n",
    "            \n",
    "            # Format the class directory name with the index\n",
    "            class_dir = f\"class_{pred_index:02d}\"\n",
    "            \n",
    "            # Create the S3 path\n",
    "            bucket_name = \"production\"\n",
    "            s3_path = f\"{bucket_name}/{class_dir}/{filename}\"\n",
    "            \n",
    "            # Upload the file to S3/MinIO\n",
    "            fs.put(img_path, s3_path)\n",
    "\n",
    "            # Store this prediction for feedback\n",
    "            prediction_id = str(uuid.uuid4())\n",
    "            current_predictions[prediction_id] = {\n",
    "                \"prediction_id\": prediction_id,\n",
    "                \"filename\": filename,\n",
    "                \"prediction\": preds,\n",
    "                \"confidence\": probs,\n",
    "                \"image_url\": f\"http://localhost:9000/production/{class_dir}/{filename}\",\n",
    "                \"class_dir\": class_dir,\n",
    "                \"sampled\" : False\n",
    "            }\n",
    "\n",
    "            store_prediction_in_tracking(fs,current_predictions[prediction_id])\n",
    "            \n",
    "            # Return the result with a flag icon for incorrect label feedback\n",
    "            result_html = f'''\n",
    "            <div style=\"display: flex; align-items: center; margin-top: 10px;\">\n",
    "                <button type=\"button\" class=\"btn btn-info btn-sm\">{preds}</button>\n",
    "                <button class=\"btn btn-sm feedback-btn\" data-prediction-id=\"{prediction_id}\" \n",
    "                        data-bs-toggle=\"tooltip\" data-bs-placement=\"top\" title=\"Flag incorrect label\"\n",
    "                        style=\"background: none; border: none; color: #dc3545; padding: 2px 0 0 8px; margin-left: 5px;\">\n",
    "                    {FLAG_SVG}\n",
    "                </button>\n",
    "            </div>\n",
    "            '''\n",
    "            \n",
    "            return result_html\n",
    "    \n",
    "    return '<a href=\"#\" class=\"badge badge-warning\">Warning</a>'\n",
    "```\n",
    "\n",
    "-   Add Feedback Route to Handle User Feedback\n",
    "\n",
    "``` python\n",
    "@app.route('/feedback', methods=['POST'])\n",
    "def feedback():\n",
    "    \"\"\"Handle user feedback about predictions\"\"\"\n",
    "    data = request.json\n",
    "    prediction_id = data.get('prediction_id')\n",
    "    \n",
    "    # Get the prediction data\n",
    "    pred_data = current_predictions[prediction_id]\n",
    "    \n",
    "    # Create user feedback task\n",
    "    task_id = create_user_feedback_task(\n",
    "        fs,\n",
    "        image_url=pred_data[\"image_url\"],\n",
    "        predicted_class=pred_data[\"prediction\"],\n",
    "        confidence=pred_data[\"confidence\"],\n",
    "        filename=pred_data[\"filename\"]\n",
    "    )\n",
    "    \n",
    "    # Return response\n",
    "    return jsonify({\n",
    "        \"success\": True,\n",
    "        \"message\": \"Thank you for your feedback!\"\n",
    "    })\n",
    "```\n",
    "\n",
    "-   Update Frontend Files and rebuild the Flask container using SSH terminal\n",
    "\n",
    "``` bash\n",
    "# Copying front end files into our flask container to update the UI to include feedback\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v1/templates/index.html /home/cc/eval-loop-chi/gourmetgram/templates/index.html\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v1/templates/base.html /home/cc/eval-loop-chi/gourmetgram/templates/base.html\n",
    "\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v1/static/js/main.js /home/cc/eval-loop-chi/gourmetgram/static/js/main.js\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v1/static/css/main.css /home/cc/eval-loop-chi/gourmetgram/static/css/main.css\n",
    "\n",
    "mkdir -p /home/cc/eval-loop-chi/gourmetgram/images/\n",
    "cp /home/cc/eval-loop-chi/images/flag-icon.svg /home/cc/eval-loop-chi/gourmetgram/images\n",
    "```\n",
    "\n",
    "``` bash\n",
    "docker-compose -f /home/cc/eval-loop-chi/docker/docker-compose-feedback.yaml up flask --build\n",
    "```\n",
    "\n",
    "#### Testing the Feedback Loop\n",
    "\n",
    "-   Go to http://{public-node-ip}:5000\n",
    "-   Upload some of the test images from the data/userfeedback/ folder\n",
    "-   Provide negative feedback for the prediction by clicking on the flag icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Check the Label Studio interface and navigate to the “User Feedback Review.” You should see the tasks have been successfully synchronized and are now available for review\n",
    "-   Please proceed to complete the labeling tasks for these User Feedback images and then sync with target storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve gathered high-quality annotations from human reviewers in Label Studio, we need to extract and structure this data for model improvement. The annotation results are currently stored as JSON files in the /labelstudio/output/ directory within our MinIO storage system.\n",
    "\n",
    "This below script does the following:\n",
    "\n",
    "-   Extracts the human-verified labels from the annotation results\n",
    "-   Retrieves the corresponding images from our production storage\n",
    "-   Organizes these images into class-specific buckets based on their corrected labels\n",
    "-   Creates a structured dataset ready for model retraining\n",
    "\n",
    "``` bash\n",
    "docker exec scheduler python3 /app/scripts/process_outputs.py\n",
    "```\n",
    "\n",
    "Navigate to the MinIO web interface at http://{public-node-ip}:9001 and inspect the cleanproduction, lowconfidence and userfeedback buckets to find the structured dataset.\n",
    "\n",
    "#### Automating workflows\n",
    "\n",
    "To maintain an efficient evaluation loop, we can automate three critical processes using cron jobs in our scheduler container:\n",
    "\n",
    "1.  Random Sampling: Automatically select representative images from production data\n",
    "2.  Storage Synchronization: Keep Label Studio and MinIO storage in sync\n",
    "3.  Output Processing: Transform completed annotations into structured training data\n",
    "\n",
    "Now, let’s establish a cron schedule within the scheduler containe to automate these processes:\n",
    "\n",
    "``` bash\n",
    "docker exec -it scheduler bash\n",
    "\n",
    "crontab -e\n",
    "```\n",
    "\n",
    "Add the following lines to run the processes on a schedule:\n",
    "\n",
    "``` bash\n",
    "0 0 * * * python /app/scripts/random_sampling.py >> /var/log/sampling.log 2>&1\n",
    "\n",
    "0 * * * * python /app/scripts/sync_script.py >> /var/log/sync.log 2>&1\n",
    "\n",
    "15 * * * * python /app/scripts/process_outputs.py >> /var/log/process.log 2>&1\n",
    "\n",
    "# Empty line at the end is required\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get explicit labels from users\n",
    "\n",
    "Our last approach leverages direct user feedback on the classifier’s predictions. When viewing their uploaded images, users can correct misclassified food items by selecting the appropriate label. This feedback may be sparse (some users won’t bother giving feedback even if the label is wrong) and noisy (some users may give incorrect feedback). This feedback loop doesn’t involve the use of Label Studio.\n",
    "\n",
    "Use the below command to modify app.py :\n",
    "\n",
    "``` bash\n",
    "nano /home/cc/eval-loop-chi/gourmetgram/app.py\n",
    "```\n",
    "\n",
    "-   Add to imports at the top of the file :\n",
    "\n",
    "``` python\n",
    "from gourmetgram_utils.feedback_tasks import create_output_json\n",
    "\n",
    "PREDICTION_TEMPLATE_PATH = os.path.join('static', 'templates', 'prediction-result.html')\n",
    "```\n",
    "\n",
    "-   Update Upload Function and create a new route `/api/classes` that returns list of classes to the frontend:\n",
    "\n",
    "``` python\n",
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        f = request.files['file']\n",
    "        filename = secure_filename(f.filename)\n",
    "        img_path = os.path.join(app.instance_path, 'uploads', filename)\n",
    "        f.save(img_path)\n",
    "       \n",
    "        preds, probs = request_fastapi(img_path)\n",
    "        if preds:\n",
    "            pred_index = np.where(classes == preds)[0][0]\n",
    "            \n",
    "            # Format the class directory name with the index\n",
    "            class_dir = f\"class_{pred_index:02d}\"\n",
    "            \n",
    "            # Create the S3 path\n",
    "            bucket_name = \"production\"\n",
    "            s3_path = f\"{bucket_name}/{class_dir}/{filename}\"\n",
    "            \n",
    "            # Upload the file to S3/MinIO\n",
    "            fs.put(img_path, s3_path)\n",
    "\n",
    "            # Store this prediction for feedback\n",
    "            prediction_id = str(uuid.uuid4())\n",
    "            current_predictions[prediction_id] = {\n",
    "                \"prediction_id\": prediction_id,\n",
    "                \"filename\": filename,\n",
    "                \"prediction\": preds,\n",
    "                \"confidence\": probs,\n",
    "                \"image_url\": f\"http://localhost:9000/production/{class_dir}/{filename}\",\n",
    "                \"class_dir\": class_dir,\n",
    "                \"sampled\": False\n",
    "            }\n",
    "\n",
    "            store_prediction_in_tracking(fs, current_predictions[prediction_id])\n",
    "            \n",
    "            # Return the result with a dropdown label and pencil icon\n",
    "            template = open(PREDICTION_TEMPLATE_PATH).read()\n",
    "            \n",
    "            result_html = template.replace(\"{prediction_id}\", prediction_id).replace(\"{prediction}\", preds)\n",
    "            \n",
    "            return result_html\n",
    "    \n",
    "    return '<a href=\"#\" class=\"badge badge-warning\">Warning</a>'\n",
    "\n",
    "@app.route('/api/classes', methods=['GET'])\n",
    "def get_classes():\n",
    "    \"\"\"Return all available classes as JSON\"\"\"\n",
    "    return jsonify(classes.tolist())\n",
    "```\n",
    "\n",
    "-   Update feedback function :\n",
    "\n",
    "``` python\n",
    "@app.route('/feedback', methods=['POST'])\n",
    "def feedback():\n",
    "    \"\"\"Handle user feedback about predictions\"\"\"\n",
    "    data = request.json\n",
    "    prediction_id = data.get('prediction_id')\n",
    "    corrected_class = data.get('corrected_class')\n",
    "    \n",
    "    if not prediction_id or not corrected_class:\n",
    "        return jsonify({\n",
    "            \"success\": False,\n",
    "            \"message\": \"Missing required parameters\"\n",
    "        }), 400\n",
    "    \n",
    "    # Get the prediction data\n",
    "    pred_data = current_predictions.get(prediction_id)\n",
    "    \n",
    "    if not pred_data:\n",
    "        return jsonify({\n",
    "            \"success\": False,\n",
    "            \"message\": \"Prediction not found!\"\n",
    "        }), 404\n",
    "    \n",
    "    if pred_data[\"prediction\"] == corrected_class:\n",
    "        return jsonify({\n",
    "            \"success\": True,\n",
    "            \"message\": \"No changes needed - class already correct\"\n",
    "        })\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        output_json_path = create_output_json(\n",
    "            fs,\n",
    "            image_url=pred_data[\"image_url\"],\n",
    "            predicted_class=pred_data[\"prediction\"],\n",
    "            corrected_class=corrected_class,\n",
    "            filename=pred_data[\"filename\"]\n",
    "        )\n",
    "        \n",
    "\n",
    "        current_predictions[prediction_id][\"prediction\"] = corrected_class\n",
    "        \n",
    "        # Calculate the new class directory\n",
    "        new_class_index = np.where(classes == corrected_class)[0][0]\n",
    "        new_class_dir = f\"class_{new_class_index:02d}\"\n",
    "        current_predictions[prediction_id][\"class_dir\"] = new_class_dir\n",
    "        \n",
    "        # Return response\n",
    "        return jsonify({\n",
    "            \"success\": True,\n",
    "            \"message\": \"Class updated successfully!\",\n",
    "            \"output_json_path\": output_json_path\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing feedback: {e}\")\n",
    "        return jsonify({\n",
    "            \"success\": False,\n",
    "            \"message\": f\"Error processing feedback: {str(e)}\"\n",
    "        }), 500\n",
    "```\n",
    "\n",
    "-   Update Frontend files and rebuild flask container in SSH terminal :\n",
    "\n",
    "``` bash\n",
    "# Copying front end files into our flask container to update the UI to include feedback\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v2/templates/index.html /home/cc/eval-loop-chi/gourmetgram/templates/index.html\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v2/templates/base.html /home/cc/eval-loop-chi/gourmetgram/templates/base.html\n",
    "\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v2/static/js/main.js /home/cc/eval-loop-chi/gourmetgram/static/js/main.js\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v2/static/js/class-feedback.js /home/cc/eval-loop-chi/gourmetgram/static/js/class-feedback.js\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v2/static/css/main.css /home/cc/eval-loop-chi/gourmetgram/static/css/main.css\n",
    "\n",
    "mkdir -p /home/cc/eval-loop-chi/gourmetgram/static/templates/\n",
    "cp /home/cc/eval-loop-chi/frontend/feedback_v2/static/templates/prediction-result.html /home/cc/eval-loop-chi/gourmetgram/static/templates/prediction-result.html\n",
    "```\n",
    "\n",
    "``` bash\n",
    "docker-compose -f /home/cc/eval-loop-chi/docker/docker-compose-feedback.yaml up flask --build\n",
    "```\n",
    "\n",
    "#### Testing the Feedback Loop\n",
    "\n",
    "-   Go to application interface at http://{public-node-ip}:5000\n",
    "-   Upload test images from the data/userfeedback/ directory\n",
    "-   Locate the pencil icon adjacent to the prediction and use it to select the correct classification from the dropdown menu\n",
    "-   Process the corrections by executing:\n",
    "\n",
    "``` bash\n",
    "docker exec scheduler python3 /app/scripts/process_outputs.py\n",
    "```\n",
    "\n",
    "-   Navigate to the MinIO web interface at http://{public-node-ip}:9001 and inspect the userfeedback2 buckets to find the structured dataset based on the user class predictions."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
